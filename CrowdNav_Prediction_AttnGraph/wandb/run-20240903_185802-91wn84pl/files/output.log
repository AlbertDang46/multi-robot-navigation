Logging to /tmp/openai-2024-09-03-18-58-05-924021
Creating dummy env object to get spaces
Loaded the following checkpoint: trained_models/my_model/holonomic/checkpoints/35600.pt
torch.Size([1, 16, 1, 32, 32])
torch.Size([1, 16, 1, 32, 32])
torch.Size([1, 16, 1, 32, 32])
/home/liyiping/anaconda3/envs/CrowdNav2/lib/python3.10/site-packages/gym/logger.py:30: UserWarning: [33mWARN: Box bound precision lowered by casting to float32
  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))
/home/liyiping/dev/ogm_pred/ogm_sogmp_trans_pos_middle_fusion/CrowdNav_Prediction_AttnGraph/train.py:125: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  actor_critic.load_state_dict(torch.load(load_path),strict=False)
/home/liyiping/dev/ogm_pred/ogm_sogmp_trans_pos_middle_fusion/CrowdNav_Prediction_AttnGraph/rl/networks/ogm_rnn.py:946: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  robot_vel_pos_list=[torch.tensor(item) for item in self.robot_vel_pos_deque[robot_index]]
/home/liyiping/dev/ogm_pred/ogm_sogmp_trans_pos_middle_fusion/CrowdNav_Prediction_AttnGraph/rl/networks/ogm_rnn.py:947: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  lidar_list=[torch.tensor(item) for item in self.lidar_deque[robot_index]]
torch.Size([2, 16, 1, 32, 32])
torch.Size([2, 16, 1, 32, 32])
torch.Size([2, 16, 1, 32, 32])
torch.Size([3, 16, 1, 32, 32])
torch.Size([3, 16, 1, 32, 32])
torch.Size([3, 16, 1, 32, 32])
torch.Size([4, 16, 1, 32, 32])
torch.Size([4, 16, 1, 32, 32])
torch.Size([4, 16, 1, 32, 32])
torch.Size([5, 16, 1, 32, 32])
torch.Size([5, 16, 1, 32, 32])
torch.Size([5, 16, 1, 32, 32])
torch.Size([6, 16, 1, 32, 32])
torch.Size([6, 16, 1, 32, 32])
torch.Size([6, 16, 1, 32, 32])
torch.Size([7, 16, 1, 32, 32])
torch.Size([7, 16, 1, 32, 32])
torch.Size([7, 16, 1, 32, 32])
torch.Size([8, 16, 1, 32, 32])
torch.Size([8, 16, 1, 32, 32])
torch.Size([8, 16, 1, 32, 32])
torch.Size([9, 16, 1, 32, 32])
torch.Size([9, 16, 1, 32, 32])
torch.Size([9, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
/home/liyiping/dev/ogm_pred/ogm_sogmp_trans_pos_middle_fusion/CrowdNav_Prediction_AttnGraph/rl/networks/ogm_rnn.py:1036: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  robot_vel_pos_list=[torch.tensor(item) for item in self.robot_vel_pos_deque[robot_index]]
/home/liyiping/dev/ogm_pred/ogm_sogmp_trans_pos_middle_fusion/CrowdNav_Prediction_AttnGraph/rl/networks/ogm_rnn.py:1037: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  lidar_list=[torch.tensor(item) for item in self.lidar_deque[robot_index]]
Predictor Training Loss: 12955.326822916666 KL Loss: 80160.48282877605 CE Loss: 12153.722005208334
Avg wmse 0.25199374556541443 Avg ssim 0.0036352435126900673
Predictor Training Loss: 11757.0830078125 KL Loss: 578.1027221679688 CE Loss: 11751.3017578125
Avg wmse 0.24569129943847656 Avg ssim 0.004574720282107592
Predictor Training Loss: 11057.9287109375 KL Loss: 1117.815205891927 CE Loss: 11046.750651041666
Avg wmse 0.24739791452884674 Avg ssim 0.004482465796172619
Predictor Training Loss: 10486.317057291666 KL Loss: 850.4756266276041 CE Loss: 10477.812174479166
Avg wmse 0.25153347849845886 Avg ssim 0.0039637740701437
Predictor Training Loss: 9886.231770833334 KL Loss: 263.60718790690106 CE Loss: 9883.595377604166
Avg wmse 0.24760407209396362 Avg ssim 0.004972890485078096
Predictor Training Loss: 9246.517252604166 KL Loss: 509.5849965413411 CE Loss: 9241.421549479166
Avg wmse 0.24888074398040771 Avg ssim 0.005454158876091242
Predictor Training Loss: 8609.75390625 KL Loss: 334.13768513997394 CE Loss: 8606.412434895834
Avg wmse 0.246310293674469 Avg ssim 0.006868091877549887
Predictor Training Loss: 8032.630696614583 KL Loss: 418.9562174479167 CE Loss: 8028.441080729167
Avg wmse 0.2512122392654419 Avg ssim 0.007077015936374664
Predictor Training Loss: 7578.074869791667 KL Loss: 770.6084594726562 CE Loss: 7570.368815104167
Avg wmse 0.25863131880760193 Avg ssim 0.006727291736751795
Predictor Training Loss: 6977.68017578125 KL Loss: 519.5858459472656 CE Loss: 6972.484375
Avg wmse 0.2590681314468384 Avg ssim 0.008483978919684887
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
torch.Size([10, 16, 1, 32, 32])
Traceback (most recent call last):
  File "/home/liyiping/dev/ogm_pred/ogm_sogmp_trans_pos_middle_fusion/CrowdNav_Prediction_AttnGraph/train.py", line 368, in <module>
    main()
  File "/home/liyiping/dev/ogm_pred/ogm_sogmp_trans_pos_middle_fusion/CrowdNav_Prediction_AttnGraph/train.py", line 232, in main
    obs, rewards, done, infos= envs.step(all_actions)
  File "/home/liyiping/dev/MARL2_check_ogm_obst_/CrowdNav_Prediction_AttnGraph/baselines/baselines/common/vec_env/vec_env.py", line 108, in step
    return self.step_wait()
  File "/home/liyiping/dev/ogm_pred/ogm_sogmp_trans_pos_middle_fusion/CrowdNav_Prediction_AttnGraph/rl/networks/envs.py", line 224, in step_wait
    obs[key] = torch.from_numpy(obs[key]).to(self.device)
RuntimeError: CUDA error: unspecified launch failure
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.